{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code is used to crawl story from some websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://americanliterature.com/100-great-short-stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# get content of html and save as text\n",
    "import requests\n",
    "url = \"https://americanliterature.com/100-great-short-stories\"\n",
    "s = requests.get(url)\n",
    "t = s.text\n",
    "\n",
    "# find all line with tag 'a'\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(t)\n",
    "head = soup.find_all('a')\n",
    "\n",
    "#get children website\n",
    "import re\n",
    "web = list()\n",
    "for i in head:\n",
    "    if re.search('<a href=\\\"/author/[a-zA-Z\\-]+/short\\-story/[a-zA-Z\\-]+\\\">[a-zA-Z\\ ]+</a>', str(i)):\n",
    "        web.append('https://americanliterature.com' + i.attrs['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # sequence number of the files\n",
    "#crawl short story from the websites\n",
    "for url in web:\n",
    "    try:\n",
    "        s = requests.get(url)\n",
    "        t = s.text\n",
    "        soup = BeautifulSoup(t)\n",
    "        title = soup.find('h1' , itemprop = 'name').get_text()\n",
    "        author = soup.find('a' , itemprop = 'author').string\n",
    "        with open('../data/data_raw/document (' + str(k) + ').txt' , 'w') as f:\n",
    "            f.write(title + '\\n\\n')\n",
    "            f.write(author + '\\n\\n')\n",
    "            for i in soup.find_all('p'):\n",
    "                f.write(i.get_text() + '\\n\\n')\n",
    "            k += 1\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://www.theguardian.com/books/2015/aug/17/the-100-best-novels-written-in-english-the-full-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of html and save as text\n",
    "import requests\n",
    "url = 'https://www.theguardian.com/books/2015/aug/17/the-100-best-novels-written-in-english-the-full-list'\n",
    "s = requests.get(url)\n",
    "t = s.text\n",
    "\n",
    "# find all line with tag 'a'\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(t)\n",
    "head = soup.find_all('a', attrs={\"data-link-name\": \"in body link\"})\n",
    "\n",
    "#get children website\n",
    "web = list()\n",
    "for i in head:\n",
    "    web.append(i.attrs['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl short story from the websites\n",
    "for url in web:\n",
    "    try:\n",
    "        s = requests.get(url)\n",
    "        t = s.text\n",
    "        soup = BeautifulSoup(t)\n",
    "        l = soup.find('h1' , itemprop = 'headline').get_text()\n",
    "        a1 = l.find('â€“') + 2\n",
    "        a2 = l.find('by') - 1\n",
    "        a3 = a2 + 4\n",
    "        a4 = l.find('(')-1\n",
    "        title = l[a1:a2]\n",
    "        author = l[a3:a4]\n",
    "        with open('../data/data_raw/document (' + str(k) + ').txt' , 'w') as f:\n",
    "            f.write(title + '\\n\\n')\n",
    "            f.write(author + '\\n\\n')\n",
    "            for i in soup.find_all('p'):\n",
    "                f.write(i.get_text() + '\\n\\n')\n",
    "            k += 1\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. https://americanliterature.com/civil-war-stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of html and save as text\n",
    "import requests\n",
    "url = 'https://americanliterature.com/civil-war-stories'\n",
    "s = requests.get(url)\n",
    "t = s.text\n",
    "\n",
    "# find all line with tag 'a'\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(t)\n",
    "head = soup.find_all('a' , href = re.compile(\"/author/\"))\n",
    "\n",
    "#get children website\n",
    "web = list()\n",
    "for i in head:\n",
    "    web.append('https://americanliterature.com' + i.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crawl short story from the websites\n",
    "for url in web[25:]:\n",
    "    try:\n",
    "        s = requests.get(url)\n",
    "        t = s.text\n",
    "        soup = BeautifulSoup(t)\n",
    "        title = soup.find('h1' , itemprop = 'name').get_text()\n",
    "        author = soup.find('a' , itemprop = 'author').string\n",
    "        with open('../data/data_raw/document (' + str(k) + ').txt' , 'w') as f:\n",
    "            f.write(title + '\\n\\n')\n",
    "            f.write(author + '\\n\\n')\n",
    "            for i in soup.find_all('p'):\n",
    "                f.write(i.get_text() + '\\n\\n')\n",
    "            k += 1\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. https://americanliterature.com/mystery-stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of html and save as text\n",
    "import requests\n",
    "url = 'https://americanliterature.com/mystery-stories'\n",
    "s = requests.get(url)\n",
    "t = s.text\n",
    "\n",
    "# find all line with tag 'a'\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(t)\n",
    "head = soup.find_all('a' , href = re.compile(\"/author/\"))\n",
    "\n",
    "#get children website\n",
    "web = list()\n",
    "for i in head:\n",
    "    web.append('https://americanliterature.com' + i.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in web[-26:]:\n",
    "    try:\n",
    "        s = requests.get(url)\n",
    "        t = s.text\n",
    "        soup = BeautifulSoup(t)\n",
    "        title = soup.find('h1' , itemprop = 'name').get_text()\n",
    "        author = soup.find('a' , itemprop = 'author').string\n",
    "        with open('../data/data_raw/document (' + str(k) + ').txt' , 'w') as f:\n",
    "            f.write(title + '\\n\\n')\n",
    "            f.write(author + '\\n\\n')\n",
    "            for i in soup.find_all('p'):\n",
    "                f.write(i.get_text() + '\\n\\n')\n",
    "            k += 1\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
